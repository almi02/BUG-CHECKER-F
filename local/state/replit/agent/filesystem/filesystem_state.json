{"file_contents":{"app.py":{"content":"from flask import Flask, render_template, request, jsonify, send_file\nimport json\nimport os\nfrom scraper_engine import AntiDetectionScraper, GoogleMapsScraper\nfrom data_exporter import DataExporter\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = os.environ.get('SESSION_SECRET', 'dev-key-change-in-production')\n\n# Initialize components\nscraper = AntiDetectionScraper()\nmaps_scraper = GoogleMapsScraper()\nexporter = DataExporter()\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/scrape', methods=['POST'])\ndef scrape_data():\n    try:\n        data = request.get_json()\n        scrape_type = data.get('type', 'url')\n        \n        if scrape_type == 'url':\n            url = data.get('url', '')\n            use_trafilatura = data.get('use_trafilatura', False)\n            \n            if not url:\n                return jsonify({'error': 'URL is required'}), 400\n            \n            result = scraper.scrape_url(url, use_trafilatura)\n            return jsonify({'status': 'success', 'data': result})\n            \n        elif scrape_type == 'google_maps':\n            query = data.get('query', '')\n            location = data.get('location', '')\n            \n            if not query:\n                return jsonify({'error': 'Search query is required'}), 400\n            \n            businesses = maps_scraper.extract_business_info(query, location)\n            return jsonify({'status': 'success', 'data': businesses, 'count': len(businesses)})\n            \n        else:\n            return jsonify({'error': 'Invalid scrape type'}), 400\n            \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/export', methods=['POST'])\ndef export_data():\n    try:\n        data = request.get_json()\n        export_data = data.get('data', [])\n        export_format = data.get('format', 'json')\n        \n        if not export_data:\n            return jsonify({'error': 'No data to export'}), 400\n        \n        if export_format == 'json':\n            filepath = exporter.export_to_json(export_data)\n        elif export_format == 'csv':\n            filepath = exporter.export_to_csv(export_data)\n        elif export_format == 'llm_jsonl':\n            filepath = exporter.export_for_llm_training(export_data, 'jsonl')\n        elif export_format == 'llm_text':\n            filepath = exporter.export_for_llm_training(export_data, 'text')\n        else:\n            return jsonify({'error': 'Invalid export format'}), 400\n        \n        return jsonify({'status': 'success', 'filepath': filepath})\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/files')\ndef list_files():\n    try:\n        files = exporter.list_exported_files()\n        return jsonify({'files': files})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/download/<path:filename>')\ndef download_file(filename):\n    try:\n        return send_file(filename, as_attachment=True)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 404\n\n@app.route('/health')\ndef health_check():\n    return jsonify({'status': 'healthy', 'message': 'AI Scraping Tool is running'})\n\nif __name__ == '__main__':\n    # Ensure scraped_data directory exists\n    os.makedirs('scraped_data', exist_ok=True)\n    \n    # Run with all hosts allowed for Replit\n    app.run(host='0.0.0.0', port=5000, debug=True)","size_bytes":3411},"data_exporter.py":{"content":"import json\nimport csv\nimport os\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\n\nclass DataExporter:\n    \"\"\"\n    Handle data export in various formats suitable for LLM training\n    \"\"\"\n    \n    def __init__(self, output_dir: str = \"scraped_data\"):\n        self.output_dir = output_dir\n        self.ensure_output_dir()\n    \n    def ensure_output_dir(self):\n        \"\"\"Create output directory if it doesn't exist\"\"\"\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n    \n    def export_to_json(self, data: List[Dict], filename: Optional[str] = None) -> str:\n        \"\"\"Export scraped data to JSON format\"\"\"\n        if filename is None:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            filename = f\"scraped_data_{timestamp}.json\"\n        \n        filepath = os.path.join(self.output_dir, filename)\n        \n        # Format data for LLM training\n        formatted_data = {\n            \"metadata\": {\n                \"exported_at\": datetime.now().isoformat(),\n                \"total_records\": len(data),\n                \"format\": \"json\"\n            },\n            \"data\": data\n        }\n        \n        with open(filepath, 'w', encoding='utf-8') as f:\n            json.dump(formatted_data, f, indent=2, ensure_ascii=False)\n        \n        print(f\"Data exported to: {filepath}\")\n        return filepath\n    \n    def export_to_csv(self, data: List[Dict], filename: Optional[str] = None) -> str:\n        \"\"\"Export scraped data to CSV format\"\"\"\n        if not data:\n            print(\"No data to export\")\n            return \"\"\n            \n        if filename is None:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            filename = f\"scraped_data_{timestamp}.csv\"\n        \n        filepath = os.path.join(self.output_dir, filename)\n        \n        # Get all unique keys from all dictionaries\n        fieldnames = set()\n        for item in data:\n            fieldnames.update(item.keys())\n        fieldnames = list(fieldnames)\n        \n        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(data)\n        \n        print(f\"Data exported to: {filepath}\")\n        return filepath\n    \n    def export_for_llm_training(self, data: List[Dict], format_type: str = \"jsonl\") -> str:\n        \"\"\"Export data in format optimized for LLM training\"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        \n        filepath = \"\"\n        \n        if format_type == \"jsonl\":\n            filename = f\"llm_training_data_{timestamp}.jsonl\"\n            filepath = os.path.join(self.output_dir, filename)\n            \n            with open(filepath, 'w', encoding='utf-8') as f:\n                for item in data:\n                    # Format each record as a single JSON line\n                    training_record = {\n                        \"input\": f\"Business information from {item.get('search_query', 'web scraping')}\",\n                        \"output\": json.dumps(item, ensure_ascii=False),\n                        \"source\": \"web_scraper\",\n                        \"timestamp\": item.get('scraped_at', datetime.now().isoformat())\n                    }\n                    f.write(json.dumps(training_record, ensure_ascii=False) + '\\n')\n        \n        elif format_type == \"text\":\n            filename = f\"llm_training_data_{timestamp}.txt\"\n            filepath = os.path.join(self.output_dir, filename)\n            \n            with open(filepath, 'w', encoding='utf-8') as f:\n                for item in data:\n                    # Format as readable text\n                    text_content = f\"Business: {item.get('name', 'Unknown')}\\n\"\n                    text_content += f\"Rating: {item.get('rating', 'No rating')}\\n\"\n                    text_content += f\"Address: {item.get('address', 'No address')}\\n\"\n                    text_content += f\"Query: {item.get('search_query', 'Unknown')}\\n\"\n                    text_content += \"---\\n\"\n                    f.write(text_content)\n        \n        print(f\"LLM training data exported to: {filepath}\")\n        return filepath\n    \n    def list_exported_files(self) -> List[str]:\n        \"\"\"List all exported files\"\"\"\n        files = []\n        if os.path.exists(self.output_dir):\n            for filename in os.listdir(self.output_dir):\n                if filename.endswith(('.json', '.csv', '.jsonl', '.txt')):\n                    files.append(os.path.join(self.output_dir, filename))\n        return files","size_bytes":4592},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"beautifulsoup4>=4.13.5\",\n    \"fake-useragent>=2.2.0\",\n    \"flask>=3.1.2\",\n    \"requests>=2.32.5\",\n    \"trafilatura>=2.0.0\",\n]\n","size_bytes":273},"replit.md":{"content":"# AI Web Scraping Tool\n\n## Overview\n\nThis is a Flask-based web scraping application designed to extract data from websites and Google Maps with anti-detection capabilities. The tool provides a web interface for performing different types of scraping operations, including general URL scraping and Google Maps business information extraction. The application is built with modular components for scraping, data export, and includes sophisticated anti-detection features to bypass bot prevention systems.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Web Interface**: Simple HTML/Bootstrap-based single-page application served via Flask templates\n- **User Interaction**: Form-based input for URLs and search queries with real-time feedback\n- **AJAX Communication**: Asynchronous requests to backend API endpoints for non-blocking user experience\n\n### Backend Architecture\n- **Framework**: Flask web framework with modular component design\n- **Core Components**:\n  - `AntiDetectionScraper`: Handles general website scraping with bot evasion\n  - `GoogleMapsScraper`: Specialized scraper for Google Maps business data\n  - `DataExporter`: Manages data export in multiple formats (JSON, CSV)\n- **API Design**: RESTful endpoints with JSON request/response format\n- **Error Handling**: Comprehensive exception handling with structured error responses\n\n### Scraping Engine Design\n- **Anti-Detection Features**: \n  - Random user agent rotation using fake-useragent library\n  - Random header generation to mimic real browser requests\n  - Rate limiting and request delays to avoid detection\n  - Proxy rotation capability (infrastructure prepared)\n  - Session management for persistent connections\n- **Content Extraction**: \n  - BeautifulSoup for HTML parsing\n  - Trafilatura integration for clean text extraction\n  - Dual extraction modes (HTML + clean text)\n\n### Data Processing Pipeline\n- **Extraction**: Multi-threaded scraping capability with retry mechanisms\n- **Processing**: Content cleaning and structuring for downstream use\n- **Export**: Multiple format support optimized for LLM training data\n- **Storage**: File-based export system with timestamped outputs\n\n## External Dependencies\n\n### Python Libraries\n- **Flask**: Web framework for API and template serving\n- **requests**: HTTP client for web scraping operations\n- **BeautifulSoup4**: HTML parsing and content extraction\n- **trafilatura**: Clean text extraction from web pages\n- **fake-useragent**: User agent rotation for anti-detection\n- **json**: Data serialization and export\n- **csv**: Structured data export functionality\n\n### Frontend Dependencies\n- **Bootstrap 5.1.3**: CSS framework for responsive UI design\n- **JavaScript (Vanilla)**: Client-side interactivity and AJAX requests\n\n### Infrastructure Requirements\n- **File System**: Local storage for scraped data exports\n- **Network Access**: HTTP/HTTPS requests to target websites\n- **Session Management**: Flask session handling with configurable secret key\n\n### Optional Integrations\n- **Proxy Services**: Infrastructure prepared for proxy rotation\n- **Google Maps API**: Potential integration for enhanced location data\n- **Database Storage**: Currently file-based but architected for database expansion","size_bytes":3308},"scraper_engine.py":{"content":"import requests\nimport random\nimport time\nimport json\nfrom fake_useragent import UserAgent\nfrom bs4 import BeautifulSoup\nimport trafilatura\nfrom typing import Dict, List, Optional\nimport threading\nfrom urllib.parse import urljoin, urlparse\n\nclass AntiDetectionScraper:\n    \"\"\"\n    Advanced web scraping engine with anti-detection features for bypassing bot prevention\n    \"\"\"\n    \n    def __init__(self):\n        self.user_agent = UserAgent()\n        self.session = requests.Session()\n        self.proxies = []  # Will be populated with proxy rotation\n        self.current_proxy_index = 0\n        self.rate_limit = 2  # Minimum seconds between requests\n        self.max_retries = 3\n        \n    def get_random_headers(self) -> Dict[str, str]:\n        \"\"\"Generate random headers to mimic real browser requests\"\"\"\n        headers = {\n            'User-Agent': self.user_agent.random,\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': random.choice([\n                'en-US,en;q=0.9',\n                'en-GB,en;q=0.9',\n                'es-ES,es;q=0.8,en;q=0.7',\n                'fr-FR,fr;q=0.8,en;q=0.7'\n            ]),\n            'Accept-Encoding': 'gzip, deflate, br',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none',\n            'Cache-Control': 'max-age=0'\n        }\n        \n        # Randomly add some additional headers\n        if random.choice([True, False]):\n            headers['Referer'] = random.choice([\n                'https://www.google.com/',\n                'https://www.bing.com/',\n                'https://duckduckgo.com/'\n            ])\n            \n        return headers\n    \n    def add_proxies(self, proxy_list: List[str]):\n        \"\"\"Add proxy servers for rotation\"\"\"\n        self.proxies = proxy_list\n        \n    def get_next_proxy(self) -> Optional[Dict[str, str]]:\n        \"\"\"Get next proxy in rotation\"\"\"\n        if not self.proxies:\n            return None\n            \n        proxy = self.proxies[self.current_proxy_index]\n        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxies)\n        \n        return {\n            'http': proxy,\n            'https': proxy\n        }\n    \n    def human_delay(self):\n        \"\"\"Add human-like delay between requests\"\"\"\n        delay = self.rate_limit + random.uniform(0.5, 3.0)\n        time.sleep(delay)\n    \n    def scrape_url(self, url: str, use_trafilatura: bool = False) -> Optional[Dict]:\n        \"\"\"\n        Scrape a single URL with anti-detection measures\n        \"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                # Get random headers and proxy\n                headers = self.get_random_headers()\n                proxies = self.get_next_proxy()\n                \n                # Make request with session\n                response = self.session.get(\n                    url,\n                    headers=headers,\n                    proxies=proxies,\n                    timeout=30,\n                    verify=True\n                )\n                \n                if response.status_code == 200:\n                    if use_trafilatura:\n                        # Use trafilatura for clean text extraction\n                        text_content = trafilatura.extract(response.text)\n                        return {\n                            'url': url,\n                            'status': 'success',\n                            'content': text_content,\n                            'method': 'trafilatura'\n                        }\n                    else:\n                        # Use BeautifulSoup for HTML parsing\n                        soup = BeautifulSoup(response.text, 'html.parser')\n                        return {\n                            'url': url,\n                            'status': 'success',\n                            'content': response.text,\n                            'title': soup.title.string if soup.title else '',\n                            'method': 'beautifulsoup'\n                        }\n                        \n                elif response.status_code == 429:\n                    # Rate limited - wait longer\n                    print(f\"Rate limited for {url}, waiting...\")\n                    time.sleep(random.uniform(10, 20))\n                    continue\n                    \n                else:\n                    print(f\"HTTP {response.status_code} for {url}\")\n                    \n            except requests.exceptions.RequestException as e:\n                print(f\"Request error for {url} (attempt {attempt + 1}): {e}\")\n                if attempt < self.max_retries - 1:\n                    time.sleep(random.uniform(5, 10))\n                    continue\n                    \n            # Add delay between attempts\n            if attempt < self.max_retries - 1:\n                self.human_delay()\n        \n        return {\n            'url': url,\n            'status': 'failed',\n            'content': None,\n            'method': None\n        }\n    \n    def bulk_scrape(self, urls: List[str], use_trafilatura: bool = False) -> List[Dict]:\n        \"\"\"\n        Scrape multiple URLs with delays and anti-detection\n        \"\"\"\n        results = []\n        \n        for i, url in enumerate(urls):\n            print(f\"Scraping {i+1}/{len(urls)}: {url}\")\n            result = self.scrape_url(url, use_trafilatura)\n            results.append(result)\n            \n            # Add human-like delay between requests\n            if i < len(urls) - 1:  # Don't delay after the last request\n                self.human_delay()\n        \n        return results\n\nclass GoogleMapsScraper(AntiDetectionScraper):\n    \"\"\"\n    Specialized scraper for Google Maps data extraction\n    \"\"\"\n    \n    def extract_business_info(self, search_query: str, location: str = \"\") -> List[Dict]:\n        \"\"\"\n        Extract business information from Google Maps search\n        \"\"\"\n        # Construct Google Maps search URL\n        base_url = \"https://www.google.com/maps/search/\"\n        query = f\"{search_query} {location}\".strip()\n        search_url = f\"{base_url}{query.replace(' ', '+')}\"\n        \n        print(f\"Searching Google Maps for: {query}\")\n        result = self.scrape_url(search_url)\n        \n        businesses = []\n        \n        if result and result['status'] == 'success' and 'soup' in result:\n            soup = result['soup']\n            \n            # Extract business listings (this is a simplified example)\n            # Google Maps uses complex JS-rendered content, this is a basic approach\n            business_elements = soup.find_all('div', attrs={'data-cid': True})\n            \n            for element in business_elements[:10]:  # Limit to first 10 results\n                try:\n                    name = element.find('span', class_='fontHeadlineSmall')\n                    name = name.text.strip() if name else \"Unknown\"\n                    \n                    rating_element = element.find('span', attrs={'aria-label': True})\n                    rating = rating_element.get('aria-label', '') if rating_element else \"\"\n                    \n                    address_element = element.find('div', class_='fontBodyMedium')\n                    address = address_element.text.strip() if address_element else \"\"\n                    \n                    business_info = {\n                        'name': name,\n                        'rating': rating,\n                        'address': address,\n                        'search_query': query,\n                        'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')\n                    }\n                    \n                    businesses.append(business_info)\n                    \n                except Exception as e:\n                    print(f\"Error extracting business info: {e}\")\n                    continue\n        \n        return businesses","size_bytes":8056}},"version":1}